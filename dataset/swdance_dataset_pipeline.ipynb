{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lryok5nOsmK7",
        "2p4Mz4npTHpB",
        "nW1of_zSgfm_",
        "M1Fx8gIFgjVG",
        "J7FPusS_Hlfy"
      ],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SWDance dataset pipeline\n",
        "From video playlist to dataset in HumanML3D format. Created and ran in Google Colaboratory, using their free GPU.\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "Before running, download [SMPL neutral v1.0.0 model for Python](https://smpl.is.tue.mpg.de/download.php) (you have to register for access). Save into `{data_path}/smpl_models/basicModel_neutral_lbs_10_207_0_v1.0.0.pkl`.\n",
        "\n",
        "\n",
        "If you want to test faster, download lower quality videos. This will result in lower quality pose estimation, but runs faster! To do this, remove `desc()` in `stream = yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first()` inside `DataProcessor.download_yt_videos()`."
      ],
      "metadata": {
        "id": "7cS-31EInvuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SetUp"
      ],
      "metadata": {
        "id": "lryok5nOsmK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t-Y8aTK7Th3",
        "outputId": "d19e0c08-7c80-48ec-88ff-b8a9c6ee0809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pip installs\n",
        "\n",
        "# # VIBE\n",
        "!pip install git+https://github.com/pytube/pytube\n",
        "!pip install tqdm yacs\n",
        "!pip install smplx\n",
        "!pip install trimesh pyrender progress filterpy\n",
        "!pip install opencv-python llvmlite\n",
        "!pip install git+https://github.com/mattloper/chumpy.git\n",
        "!pip install transforms3d\n",
        "!pip install smplpytorch\n",
        "\n",
        "# whisper\n",
        "!pip install ffmpeg moviepy\n",
        "!pip install -U stable-ts # stable whisper with better timestamps\n",
        "\n",
        "# for animation:\n",
        "!pip install matplotlib==3.3.4\n",
        "\n",
        "# for emotion detection\n",
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "\n",
        "# for downloading stuff\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "id": "szU2ZOU4f5yQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install VIBE and yolov7\n",
        "vibe_path = '/content' # @param {type:\"string\"}\n",
        "%cd {vibe_path}\n",
        "!git clone https://github.com/mkocabas/VIBE.git\n",
        "%cd VIBE\n",
        "!source scripts/prepare_data.sh\n",
        "\n",
        "\n",
        "yolo_path = '/content' # @param {type:\"string\"}\n",
        "%cd {yolo_path}\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd yolov7\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
        "\n",
        "%cd /content/"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WavlJ6Ta7_Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fix pytube bug\n",
        "import os\n",
        "import inspect\n",
        "import pytube\n",
        "\n",
        "package_path = os.path.dirname(inspect.getfile(pytube))\n",
        "pytube_path = f'{package_path}/innertube.py'\n",
        "\n",
        "with open(pytube_path, 'r', encoding='utf-8') as file:\n",
        "    data = file.readlines()\n",
        "\n",
        "print(data[222])\n",
        "data[222] = data[222].replace('ANDROID_MUSIC', 'ANDROID')\n",
        "data[222] = data[222].replace('ANDROID', 'IOS')\n",
        "\n",
        "with open(pytube_path, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(data)\n",
        "\n",
        "# test\n",
        "with open(pytube_path, 'r', encoding='utf-8') as file:\n",
        "    data = file.readlines()\n",
        "print(data[222])\n",
        "\n",
        "\n",
        "## for the other thingy\n",
        "\n",
        "pytube_path = f'{package_path}/cipher.py'\n",
        "with open(pytube_path, 'r', encoding='utf-8') as file:\n",
        "    data = file.readlines()\n",
        "\n",
        "print(data[410])\n",
        "data[410] = data[410].replace('find_object_from_startpoint(raw_code, match.span()[1] - 1)', 'js')\n",
        "\n",
        "with open(pytube_path, 'w', encoding='utf-8') as file:\n",
        "    file.writelines(data)\n",
        "\n",
        "# test\n",
        "with open(pytube_path, 'r', encoding='utf-8') as file:\n",
        "    data = file.readlines()\n",
        "print(data[410])"
      ],
      "metadata": {
        "id": "byKzRSiTtEOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bf7320-c7ac-4017-843e-a17fb245690c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def __init__(self, client='IOS', use_oauth=False, allow_cache=True):\n",
            "\n",
            "    def __init__(self, client='IOS', use_oauth=False, allow_cache=True):\n",
            "\n",
            "    transform_plan_raw = js\n",
            "\n",
            "    transform_plan_raw = js\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fix libcuda error\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DalyK-8rmhmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Pipeline"
      ],
      "metadata": {
        "id": "tvgjVNVW5cZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings\n",
        "import os\n",
        "\n",
        "data_dir = '/content/SWDance' # @param {type:\"string\"}\n",
        "video_path = '/tmp/videos' # @param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "if not os.path.exists(video_path):\n",
        "    os.makedirs(video_path)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nHOwBEmV6il9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "%cd /content\n",
        "# %cd /content/drive/MyDrive/\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.io import json\n",
        "\n",
        "import time\n",
        "import joblib\n",
        "import os, sys\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import codecs as cs\n",
        "import shutil\n",
        "\n",
        "# captions\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import stable_whisper\n",
        "import nltk\n",
        "\n",
        "# videos\n",
        "import cv2\n",
        "import moviepy as mopy\n",
        "from moviepy import editor\n",
        "from pytube import Playlist, YouTube\n",
        "from torch.utils.data import DataLoader\n",
        "from smplx import SMPL as SMPL_native\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "# loading models...\n",
        "print('Loading whisper model...')\n",
        "WHISPER_MODEL = stable_whisper.load_model('base')\n",
        "\n",
        "print('Loading models for caption augmentation...')\n",
        "import gensim.downloader as gensim_api\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "GLOVE_MODEL = gensim_api.load(\"glove-wiki-gigaword-100\")\n",
        "EMOTION_TOKENIZER = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "EMOTION_MODEL = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-emotion\")\n",
        "\n",
        "print(\"Loading SMPL layer...\") # for transform to smpl24\n",
        "import transforms3d\n",
        "from smplpytorch.display_utils import display_model\n",
        "from smplpytorch.pytorch.smpl_layer import SMPL_Layer\n",
        "SMPL_LAYER = SMPL_Layer(center_idx=0,gender='neutral',model_root=osp.join(data_dir, 'smpl_models'))\n",
        "\n",
        "NLP = spacy.load('en_core_web_sm')\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "\n",
        "print('Loading VIBE model...')\n",
        "sys.path.append(osp.join(vibe_path, 'VIBE'))\n",
        "# sys.path.append(osp.join(vibe_path, 'VIBE/data/vibe_data'))\n",
        "# sys.path.append('/content/drive/MyDrive/THESIS/VIBE/')\n",
        "# print(sys.path)\n",
        "\n",
        "%cd VIBE\n",
        "from lib.models.vibe import VIBE_Demo\n",
        "from lib.utils.demo_utils import download_ckpt, video_to_images\n",
        "from lib.dataset.inference import Inference\n",
        "from lib.models.smpl import SMPL_MODEL_DIR\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "VIBE_MODEL = VIBE_Demo(seqlen=16, n_layers=2, hidden_size=1024,\n",
        "                                add_linear=True, use_residual=True).to(DEVICE)\n",
        "pretrained_file = download_ckpt(use_3dpw=False)\n",
        "ckpt = torch.load(pretrained_file)\n",
        "print(f'Performance of pretrained VIBE model on 3DPW: {ckpt[\"performance\"]}')\n",
        "\n",
        "ckpt = ckpt['gen_state_dict']\n",
        "VIBE_MODEL.load_state_dict(ckpt, strict=False)\n",
        "VIBE_MODEL.eval()\n",
        "print(f'Loaded pretrained VIBE weights from \\\"{pretrained_file}\\\"')\n",
        "\n",
        "\n",
        "# utils\n",
        "def get_filename(file_idx):\n",
        "  name_length = 6\n",
        "  diff = name_length - len(str(file_idx))\n",
        "  return diff*'0' + str(file_idx)\n",
        "\n",
        "# credit: HumanML3D\n",
        "def swap_left_right(joints):\n",
        "\n",
        "  assert len(joints.shape) == 3 and joints.shape[-1] == 3\n",
        "  joints = joints.copy()\n",
        "  joints[..., 0] *= -1\n",
        "\n",
        "  right_chain = [2, 5, 8, 11, 14, 17, 19, 21]\n",
        "  left_chain = [1, 4, 7, 10, 13, 16, 18, 20]\n",
        "\n",
        "  tmp = joints[:, right_chain]\n",
        "  joints[:, right_chain] = joints[:, left_chain]\n",
        "  joints[:, left_chain] = tmp\n",
        "  return joints\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU8u1Wq3DKE3",
        "outputId": "24bc781d-386f-4295-c0f8-ca8eb383adc2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Loading whisper model...\n",
            "Loading models for caption augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1509: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/smplpytorch/pytorch/smpl_layer.py:41: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  torch.Tensor(smpl_data['betas'].r).unsqueeze(0))\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SMPL layer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VIBE model...\n",
            "/content/VIBE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "=> loaded pretrained model from 'data/vibe_data/spin_model_checkpoint.pth.tar'\n",
            "Performance of pretrained VIBE model on 3DPW: 56.56075477600098\n",
            "Loaded pretrained VIBE weights from \"data/vibe_data/vibe_model_wo_3dpw.pth.tar\"\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Single Person Detector (based on yolov7)\n",
        "# NOTE: I changed line 21 in utils/google_utils to `file = Path(str(file))` (in Drive the uppercase matters...)\n",
        "\n",
        "from pathlib import Path\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "print('yolo imports...')\n",
        "sys.path.append(yolo_path+'/yolov7')\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "def load_yolo_model(weights=[yolo_path+'/yolov7/yolov7.pt'], img_size=640, trace=True, device=''):\n",
        "\n",
        "  device = select_device(device)\n",
        "  half = device.type != 'cpu'\n",
        "\n",
        "  # Load model\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  if trace:\n",
        "      model = TracedModel(model, device, img_size)\n",
        "  if half:\n",
        "      model.half()  # to FP16\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "YOLO_MODEL = load_yolo_model()\n",
        "\n",
        "def SinglePersonDetector(\n",
        "    model, source, img_size=640, conf_thres=0.25,\n",
        "    iou_thres=0.45, device='', view_img=False, save_txt=True, save_conf=False,\n",
        "    nosave=False, classes=None, agnostic_nms=False, augment=False,\n",
        "    update=False, project='runs/detect', name='exp', exist_ok=False, save_bboxes=True):\n",
        "\n",
        "  imgsz=img_size\n",
        "\n",
        "  save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
        "  webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
        "      ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "\n",
        "  # Directories\n",
        "  save_dir = Path(increment_path(Path(project) / name, exist_ok=exist_ok))  # increment run\n",
        "  (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "  # Initialize\n",
        "  set_logging()\n",
        "  device = select_device(device)\n",
        "  half = device.type != 'cpu'\n",
        "\n",
        "  # Model attributes\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "\n",
        "  # Second-stage classifier\n",
        "  classify = False\n",
        "  if classify:\n",
        "      modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
        "      modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
        "\n",
        "  # Set Dataloader\n",
        "  vid_path, vid_writer = None, None\n",
        "  if webcam:\n",
        "      view_img = check_imshow()\n",
        "      cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "      dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
        "  else:\n",
        "      dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
        "\n",
        "  # Get names and colors\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "\n",
        "  # Run inference\n",
        "  if device.type != 'cpu':\n",
        "      model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "  old_img_w = old_img_h = imgsz\n",
        "  old_img_b = 1\n",
        "\n",
        "  t0 = time.time()\n",
        "  person_tracked = False\n",
        "  bbox_info = {'frames':[], 'bbox': []}\n",
        "  for path, img, im0s, vid_cap in tqdm(dataset):\n",
        "      img = torch.from_numpy(img).to(device)\n",
        "      img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "      if img.ndimension() == 3:\n",
        "          img = img.unsqueeze(0)\n",
        "\n",
        "      # Warmup\n",
        "      if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
        "          old_img_b = img.shape[0]\n",
        "          old_img_h = img.shape[2]\n",
        "          old_img_w = img.shape[3]\n",
        "          for i in range(3):\n",
        "              model(img, augment=augment)[0]\n",
        "\n",
        "      # Inference\n",
        "      t1 = time_synchronized()\n",
        "      with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
        "          pred = model(img, augment=augment)[0]\n",
        "      t2 = time_synchronized()\n",
        "\n",
        "      # Apply NMS\n",
        "      pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
        "      t3 = time_synchronized()\n",
        "\n",
        "      # Apply Classifier\n",
        "      if classify:\n",
        "          pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "      # Process detections\n",
        "      for i, det in enumerate(pred):  # detections per image\n",
        "          if webcam:  # batch_size >= 1\n",
        "              p = Path(path[i])\n",
        "              s, im0, frame = '%g: ' % i, im0s[i].copy(), dataset.count\n",
        "          else:\n",
        "              p = Path(path)\n",
        "              s, im0, frame = '', im0s, getattr(dataset, 'frame', int(p.stem)-1)\n",
        "\n",
        "          save_path = str(save_dir / p.name)  # img.jpg\n",
        "          txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
        "          gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "\n",
        "          if len(det):\n",
        "              # Rescale boxes from img_size to im0 size\n",
        "              det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "              # Print results\n",
        "              for c in det[:, -1].unique():\n",
        "                  n = (det[:, -1] == c).sum()  # detections per class\n",
        "                  s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "\n",
        "              # Results\n",
        "              biggest_bbox = 0\n",
        "              xyxy_main = det[-1][:4]\n",
        "\n",
        "              for *xyxy, conf, cls in reversed(det):  # check all boxes & get main person out\n",
        "                if cls == 0:  # get persons (idx for person = 0)\n",
        "                  person_tracked = True\n",
        "\n",
        "                  # MPT style\n",
        "                  xyxy = [d.cpu() for d in xyxy]\n",
        "                  w, h = xyxy[2] - xyxy[0], xyxy[3] - xyxy[1]\n",
        "                  c_x, c_y = xyxy[0] + w/2, xyxy[1] + h/2\n",
        "\n",
        "                  if float(w*h) > biggest_bbox:\n",
        "                    biggest_bbox = float(w*h)\n",
        "                    w = h = np.where(w / h > 1, w, h)\n",
        "                    bbox_MPT_main = np.array([c_x, c_y, w, h])\n",
        "                    xyxy_main = xyxy\n",
        "\n",
        "              if person_tracked:   # only write if a person has been tracked...\n",
        "                bbox_info['frames'] += [frame]\n",
        "                bbox_info['bbox'] += [bbox_MPT_main]\n",
        "\n",
        "                if save_txt:  # Write to file\n",
        "                    line = (cls, *bbox_MPT_main, conf) if save_conf else (cls, *bbox_MPT_main)  # label format\n",
        "                    with open(txt_path + '.txt', 'a') as f:\n",
        "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                if save_img or view_img:  # Add bbox to image\n",
        "                    label = f'{names[int(cls)]} {conf:.2f}'\n",
        "                    plot_one_box(xyxy_main, im0, label=label, color=colors[int(cls)], line_thickness=1)\n",
        "\n",
        "          # Print time (inference + NMS)\n",
        "          # print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
        "\n",
        "          # Stream results\n",
        "          if view_img and person_tracked:\n",
        "              cv2.imshow(str(p), im0)\n",
        "              cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "          # Save results (image with detections)\n",
        "          if save_img and person_tracked:\n",
        "              if dataset.mode == 'image':\n",
        "                  cv2.imwrite(save_path, im0)\n",
        "                  print(f\" The image with the result is saved in: {save_path}\")\n",
        "              else:  # 'video' or 'stream'\n",
        "                  if vid_path != save_path:  # new video\n",
        "                      vid_path = save_path\n",
        "                      if isinstance(vid_writer, cv2.VideoWriter):\n",
        "                          vid_writer.release()  # release previous video writer\n",
        "                      if vid_cap:  # video\n",
        "                          fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                          w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                          h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                      else:  # stream\n",
        "                          fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                          save_path += '.mp4'\n",
        "                      vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                  vid_writer.write(im0)\n",
        "\n",
        "  if save_txt or save_img and person_tracked:\n",
        "      s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "      print(f\"Results saved to {save_dir}{s}\")\n",
        "\n",
        "  if not person_tracked:\n",
        "    shutil.rmtree(save_dir)\n",
        "    print(\"No person was tracked so no output :(\")\n",
        "\n",
        "  print(f'Done. ({time.time() - t0:.3f}s)')\n",
        "  return bbox_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4P-YK13xiqm",
        "outputId": "a9b9f059-634c-4285-e4a1-d0e02b2bbccc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolo imports...\n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:844: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if param.grad is not None:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DataProcessor\n",
        "\n",
        "class DataProcessor:\n",
        "\n",
        "  def __init__(self, data_dir, device=None):\n",
        "    ''' In video_path, insert either a directory where your videos are stored,\n",
        "        or otherwise, a link to a single youtube video or a youtube playlist\n",
        "        link '''\n",
        "    self.data_dir = data_dir\n",
        "    self.video_path = video_path\n",
        "    self.videos_df = pd.read_csv(osp.join(self.data_dir, 'video_files.csv'))\n",
        "\n",
        "    self.device = DEVICE if device is None else device\n",
        "\n",
        "    self.whisper_model = WHISPER_MODEL\n",
        "    self.glove_model = GLOVE_MODEL\n",
        "    self.emotion_tokenizer = EMOTION_TOKENIZER\n",
        "    self.emotion_model = EMOTION_MODEL\n",
        "    self.vibe_model = VIBE_MODEL\n",
        "    self.SMPL_layer = SMPL_LAYER\n",
        "    self.nlp = NLP\n",
        "\n",
        "    self.min_frames = 25\n",
        "    self.vibe_batch_size = 128\n",
        "    self.tracker_batch_size = 12\n",
        "    self.yolo_img_size = 416\n",
        "    self.display = False\n",
        "    self.render = False\n",
        "    self.bbox_scale = 1.1\n",
        "    self.smooth_pose = False\n",
        "\n",
        "  def download_yt_video(self, url, video_idx):\n",
        "\n",
        "    if osp.isfile(f'{self.video_path}/{video_idx}.mp4'):\n",
        "      print(f'yt video exists: {self.video_path}/{video_idx}.mp4')\n",
        "      return f'{self.video_path}/{video_idx}.mp4', self.videos_df.loc[video_idx, ('fps')]\n",
        "\n",
        "    yt = YouTube(url, use_oauth=True)\n",
        "    # stream = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
        "    stream = yt.streams.filter(file_extension='mp4').order_by('resolution').desc().first()\n",
        "    print(f'video resolution= {stream.resolution}')\n",
        "    video_fps = stream.fps\n",
        "    video_file = stream.download(output_path=f'{self.video_path}', filename=f'{video_idx}.mp4')\n",
        "\n",
        "    audio_stream = yt.streams.filter(type='audio').desc().first()\n",
        "    audio_file = audio_stream.download(output_path='/tmp', filename=f'{video_idx}.wav')\n",
        "\n",
        "    if video_file is None:\n",
        "      exit('Youtube url is not valid!')\n",
        "    print(f'YouTube Video has been downloaded to {video_file}...')\n",
        "\n",
        "    if audio_file is None:\n",
        "      exit('Youtube url is not valid!')\n",
        "    print(f'Audio has been downloaded to {audio_file}...')\n",
        "\n",
        "    return video_file, video_fps\n",
        "\n",
        "  def get_whisper_captions(self, video_file):\n",
        "    video_idx = osp.basename(video_file).replace('.mp4', '')\n",
        "    audio_file = osp.join('/tmp', f'{video_idx}.wav')\n",
        "\n",
        "    caps_dir = osp.join(self.data_dir, 'caption_output')\n",
        "    Path(caps_dir).mkdir(parents=True, exist_ok=True)\n",
        "    caps_file = osp.join(caps_dir, f\"{video_idx}.pkl\")\n",
        "\n",
        "    if osp.isfile(audio_file):\n",
        "      print(f'audio video exists: {audio_file}')\n",
        "    else:\n",
        "      clip = mopy.editor.VideoFileClip(video_file)\n",
        "      clip.audio.write_audiofile(audio_file, codec='pcm_s16le')\n",
        "\n",
        "    if osp.isfile(caps_file):\n",
        "      print(f'caps file exists: {caps_file}')\n",
        "      return joblib.load(caps_file)\n",
        "    else:\n",
        "      captions = self.whisper_model.transcribe(audio_file, word_timestamps=True).to_dict()\n",
        "      joblib.dump(captions, caps_file)\n",
        "\n",
        "    del audio_file\n",
        "    return captions\n",
        "\n",
        "  def process_captions(self, captions):\n",
        "    # function inspired by HumanML3D github\n",
        "\n",
        "    processed_captions = []\n",
        "\n",
        "    for caption in captions['segments']:\n",
        "      text = caption['text']\n",
        "      sentences = self.augment_captions(text).split('\\n')\n",
        "      tokens = []\n",
        "\n",
        "      for text in sentences:\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        if doc == \"\" or doc == \" \":\n",
        "          # save empty as X -> https://spacy.io/api/token#attributes\n",
        "          tokens += [' /X']\n",
        "\n",
        "        else:\n",
        "          word_list, pos_list = [], []\n",
        "          for token in doc:\n",
        "            word = token.text\n",
        "            if not word.isalpha():\n",
        "              continue\n",
        "            if (token.pos_ == 'NOUN' or token.pos_ == 'VERB') and (word != 'left'):\n",
        "              word_list.append(token.lemma_)\n",
        "            else:\n",
        "              word_list.append(word)\n",
        "            pos_list.append(token.pos_)\n",
        "          tokens += [' '.join([f'{word_list[i]}/{pos_list[i]}' for i in range(len(word_list))])]\n",
        "\n",
        "      # timestamp (s) to frame_count\n",
        "      start = caption['start']\n",
        "      end = caption['end']\n",
        "\n",
        "      caption_info = {\"text\": sentences, \"tokens\": tokens, \"start\": start, \"end\": end}\n",
        "      processed_captions.append(caption_info)\n",
        "\n",
        "    return processed_captions\n",
        "\n",
        "  def augment_captions(self, caption, emotions=True, similar_words=True):\n",
        "\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    cap_polarity = sia.polarity_scores(caption)['compound']\n",
        "\n",
        "    if emotions:\n",
        "      input_ids = self.emotion_tokenizer.encode(caption, return_tensors='pt')\n",
        "      emotion_output = self.emotion_model.generate(input_ids=input_ids, max_length=20)\n",
        "      emotion = [self.emotion_tokenizer.decode(ids, skip_special_tokens=True) for ids in emotion_output][0]\n",
        "    else:\n",
        "      emotion = \"\"\n",
        "\n",
        "    if similar_words:   # TODO: only simwords for the emotion word??\n",
        "      try:\n",
        "        similar_words = GLOVE_MODEL.most_similar(emotion) # could premake this bc it has only 5 emotions\n",
        "        similar_words = [w[0] for w in similar_words]\n",
        "        emotion_polarity = sia.polarity_scores(emotion)['compound']\n",
        "        same_polarity_words = []\n",
        "\n",
        "        for word in similar_words:\n",
        "          word_polarity = sia.polarity_scores(word)['compound']\n",
        "\n",
        "          if emotion_polarity - 0.2 <= word_polarity <= emotion_polarity + 0.2:\n",
        "            same_polarity_words += [word]\n",
        "        same_polarity_words = \" \".join(same_polarity_words[:5])\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Didn't work for emotion {emotion}, {e}\")\n",
        "        same_polarity_words = \"\"\n",
        "    else:\n",
        "      same_polarity_words = \"\"\n",
        "\n",
        "    return f'{caption}\\n{caption} {emotion} {same_polarity_words}\\n{emotion} {same_polarity_words}'\n",
        "\n",
        "  def get_vibe_poses(self, video_file):\n",
        "    # inspired by VIBE github code but modified.\n",
        "    # need to download VIBE in subfolder to be able to run this.\n",
        "\n",
        "    video_idx = osp.basename(video_file).replace('.mp4', '')\n",
        "    vibe_dir = osp.join(self.data_dir, 'vibe_output')\n",
        "    Path(vibe_dir).mkdir(parents=True, exist_ok=True)\n",
        "    output_file = osp.join(vibe_dir, f\"{video_idx}.pkl\")\n",
        "\n",
        "    if osp.isfile(output_file):\n",
        "      print(f'VIBE file exists: {output_file}')\n",
        "      output_dict = joblib.load(output_file)\n",
        "      return output_dict\n",
        "\n",
        "    if osp.isdir(f'/tmp/{video_idx}_mp4'):\n",
        "      image_folder = f'/tmp/{video_idx}_mp4'\n",
        "      img_files = [i for i in os.listdir(image_folder)]\n",
        "      num_frames = len(img_files)\n",
        "      test_img = cv2.imread(f'{image_folder}/{img_files[0]}')\n",
        "      img_shape = test_img.shape\n",
        "    else:\n",
        "      image_folder, num_frames, img_shape = video_to_images(video_file, return_info=True)\n",
        "\n",
        "    print(f'Video {video_idx}, number of frames: {num_frames}')\n",
        "    orig_height, orig_width = img_shape[:2]\n",
        "    total_time = time.time()\n",
        "\n",
        "    print('Tracking pose...')\n",
        "    tracking_results = SinglePersonDetector(\n",
        "        YOLO_MODEL, source=image_folder,\n",
        "        conf_thres=0.85, save_txt=False,\n",
        "        save_conf=False, nosave=True,\n",
        "        project='data/SWDance/videos',\n",
        "    )\n",
        "\n",
        "    bboxes = np.array(tracking_results['bbox'])\n",
        "    frames = np.array(tracking_results['frames'])\n",
        "\n",
        "    print(f'Running VIBE...')\n",
        "    dataset = Inference(\n",
        "        image_folder=image_folder,\n",
        "        frames=frames,\n",
        "        bboxes=bboxes,\n",
        "        joints2d=None,\n",
        "        scale=self.bbox_scale,\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=self.vibe_batch_size, num_workers=4)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred_cam, pred_verts, pred_pose, pred_betas, pred_joints3d = [], [], [], [], []\n",
        "\n",
        "      for batch in tqdm(dataloader):\n",
        "        try:\n",
        "          batch = batch.unsqueeze(0)\n",
        "          batch = batch.to(self.device)\n",
        "          batch_size, seqlen = batch.shape[:2]\n",
        "\n",
        "          output = dp.vibe_model(batch)[-1]\n",
        "\n",
        "          pred_cam.append(output['theta'][:, :, :3].reshape(batch_size * seqlen, -1))\n",
        "          pred_verts.append(output['verts'].reshape(batch_size * seqlen, -1, 3))\n",
        "          pred_pose.append(output['theta'][:,:,3:75].reshape(batch_size * seqlen, -1))\n",
        "          pred_betas.append(output['theta'][:, :,75:].reshape(batch_size * seqlen, -1))\n",
        "          pred_joints3d.append(output['kp_3d'].reshape(batch_size * seqlen, -1, 3))\n",
        "\n",
        "        except Exception as e:\n",
        "          print(\"Error:\", e)\n",
        "\n",
        "        del batch\n",
        "\n",
        "    print('Transform pose to SMPL24 ...')\n",
        "    smpl24_joints = self.transform_pose_smpl24(torch.cat(pred_pose, dim=0).cpu(),\n",
        "                                               torch.cat(pred_betas, dim=0).cpu())\n",
        "    print('Post process pose and smooth...')\n",
        "    smpl24_joints_smooth = self.post_process_pose(dataset.bboxes, smpl24_joints)\n",
        "\n",
        "    output_dict = {\n",
        "            'pred_cam': torch.cat(pred_cam, dim=0).cpu().numpy(),\n",
        "            'verts': torch.cat(pred_verts, dim=0).cpu().numpy(),\n",
        "            'pose': torch.cat(pred_pose, dim=0).cpu().numpy(),\n",
        "            'betas': torch.cat(pred_betas, dim=0).cpu().numpy(),\n",
        "            'joints3d': torch.cat(pred_joints3d, dim=0).cpu().numpy(),\n",
        "            'bboxes': dataset.bboxes,\n",
        "            'frame_ids': dataset.frames,\n",
        "            'smpl24_joints': smpl24_joints,\n",
        "            'smpl24_joints_smooth': smpl24_joints_smooth\n",
        "            }\n",
        "\n",
        "    total_time = time.time() - total_time\n",
        "    print(f'Total time: {total_time:.2f}s for {num_frames} frames.')\n",
        "\n",
        "    print(f'Saving output results to \\\"{output_file}\\\".')\n",
        "    joblib.dump(output_dict, output_file)\n",
        "    # shutil.rmtree(image_folder)   # deletes the images\n",
        "    return output_dict\n",
        "\n",
        "  def transform_pose_smpl24(self, pose, betas):\n",
        "    # get smpl24\n",
        "    verts, smpl24_joints = self.SMPL_layer(pose, th_betas=betas)\n",
        "\n",
        "    # flip upside down (VIBE output = upside down)\n",
        "    R = transforms3d.euler.axangle2mat([1, 0, 0], -np.pi)\n",
        "    smpl24_rotated = np.dot(smpl24_joints, R)\n",
        "\n",
        "    return smpl24_rotated\n",
        "\n",
        "  def align_captions_poses(self, captions, pose_dict, index_df, video_idx):\n",
        "\n",
        "    print(f'Aligning poses and caps for video {video_idx}...')\n",
        "    file_idx = 0 if index_df.empty else index_df['file_idx'].iloc[-1]+1\n",
        "    curr_timestamp = 0\n",
        "\n",
        "    for caption in captions:\n",
        "      if caption['start'] > curr_timestamp:\n",
        "        # frames with NO captions\n",
        "        file_idx, index_df = self.save_pose(\n",
        "            file_idx, video_idx, pose_dict, index_df,\n",
        "            start=curr_timestamp,\n",
        "            end=caption['start'],\n",
        "            caption=False)\n",
        "\n",
        "      # frames WITH captions\n",
        "      file_idx, index_df = self.save_pose(\n",
        "          file_idx, video_idx, pose_dict, index_df,\n",
        "          start=caption['start'],\n",
        "          end=caption['end'],\n",
        "          caption=caption)\n",
        "      curr_timestamp = caption['end']\n",
        "\n",
        "    return index_df\n",
        "\n",
        "  def fill_missing_frames(self, joints):\n",
        "\n",
        "    consec_missing = []   # TODO: fill up with in-betweening??\n",
        "\n",
        "    for i in range(len(joints)):\n",
        "      if np.isnan(joints[i]).all():\n",
        "        consec_missing += [i]\n",
        "        if len(consec_missing) > 10:\n",
        "          print('more than 10 are missing consecutively ', i)\n",
        "        joints[i] = joints[i-1]  # fill with prev joints\n",
        "      else:\n",
        "        consec_missing = []\n",
        "\n",
        "    return joints\n",
        "\n",
        "  def save_pose(self, file_idx, video_idx, pose_dict, index_df, start, end, caption=True):\n",
        "    filename = get_filename(file_idx)\n",
        "    video_fps = self.videos_df.loc[video_idx, ('fps')]\n",
        "    frame_ids = [i for i in range(round(start*video_fps), round(end*video_fps)+1)]\n",
        "    joints = []\n",
        "\n",
        "    for frame_id in frame_ids:\n",
        "      idx = np.where(pose_dict['frame_ids'] == frame_id)[0] # first appearance\n",
        "      try:\n",
        "        if idx.size > 0:\n",
        "          joints += [pose_dict['smpl24_joints_smooth'][idx[0]]]\n",
        "        else:\n",
        "          joints += [np.full([24, 3], np.nan)]\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"not working for joints idx {idx}, {type(joints)} to append {type(pose_dict['smpl24_joints_smooth'][idx[0]])} \\n {e}\")\n",
        "\n",
        "    if all(np.isnan(j).all() for j in joints):\n",
        "      print(f'no joints for video {video_idx} at {start} to {end}')\n",
        "      return file_idx, index_df\n",
        "\n",
        "    # remove the Nones at the beginning and end of the list\n",
        "    while np.isnan(joints[-1]).all():\n",
        "      end -= round(1/video_fps, 2)\n",
        "      del joints[-1]\n",
        "\n",
        "    while np.isnan(joints[0]).all():\n",
        "      start += round(1/video_fps, 2)\n",
        "      del joints[0]\n",
        "\n",
        "    if len(joints) < self.min_frames:\n",
        "      print(f'not enough joints for video {video_idx} at {start} to {end}')\n",
        "      return file_idx, index_df\n",
        "\n",
        "    joints = self.fill_missing_frames(joints)\n",
        "    joints = np.stack(joints, axis=0)\n",
        "\n",
        "    for side in ['', 'M']: # mirror pose\n",
        "      if side == 'M':\n",
        "        joints = swap_left_right(joints)\n",
        "\n",
        "      joints_dir = osp.join(self.data_dir, 'joints')\n",
        "      Path(joints_dir).mkdir(parents=True, exist_ok=True)\n",
        "      np.save(osp.join(joints_dir, f'{side}{filename}.npy'), joints)\n",
        "\n",
        "      df_row = {'file_idx': file_idx,\n",
        "                'video_idx': video_idx,               # which video it belongs to\n",
        "                'start_frame': round(start*video_fps),\n",
        "                'end_frame': round(end*video_fps),\n",
        "                'start_time': start,\n",
        "                'end_time': end,\n",
        "                'new_name': f'{side}{filename}.npy',  # named for humanml3d\n",
        "                'fps': video_fps,\n",
        "                'caption': \"\" if not caption else caption[\"text\"][0],\n",
        "                'no_frames': len(joints)}\n",
        "      index_df = pd.concat([index_df, pd.DataFrame([df_row])], ignore_index=True)\n",
        "\n",
        "      text_dir = osp.join(self.data_dir, 'texts')\n",
        "      os.makedirs(text_dir, exist_ok=True)\n",
        "      tokens_dir = osp.join(self.data_dir, 'tokens')\n",
        "      os.makedirs(tokens_dir, exist_ok=True)\n",
        "      just_texts_dir = osp.join(self.data_dir, 'just_texts')\n",
        "      os.makedirs(just_texts_dir, exist_ok=True)\n",
        "\n",
        "      if caption:\n",
        "        # with open(osp.join(text_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "        #   f.write(f'{caption[\"text\"]}#{caption[\"tokens\"]}#0.0#0.0\\n')\n",
        "        with open(osp.join(text_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "          for i, t in enumerate(caption[\"text\"]):\n",
        "            f.write(f'{t}#{caption[\"tokens\"][i]}#0.0#0.0\\n')\n",
        "        with open(osp.join(tokens_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "          f.write('\\n'.join(caption[\"tokens\"]))\n",
        "        with open(osp.join(just_texts_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "          f.write('\\n'.join(caption[\"text\"]))\n",
        "\n",
        "      else:\n",
        "        with open(osp.join(text_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "          f.write(f' # /X#0.0#0.0\\n # /X#0.0#0.0\\n # /X#0.0#0.0\\n')\n",
        "        with open(osp.join(tokens_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "          f.write(' /X\\n /X\\n /X\\n')\n",
        "        with open(osp.join(just_texts_dir, f'{side}{filename}.txt'), 'w') as f:\n",
        "          f.write(f' \\n \\n \\n')\n",
        "\n",
        "    file_idx += 1\n",
        "\n",
        "    return file_idx, index_df\n",
        "\n",
        "  def post_process_pose(self, bboxes, joints):\n",
        "    # first, get x,y movement from bboxes\n",
        "    # normalise bboxes between 0 & 1\n",
        "    bbox_diff = np.max(bboxes, axis=0) - np.min(bboxes, axis=0)\n",
        "    bbox_norm = (bboxes-np.min(bboxes, axis=0))/bbox_diff\n",
        "\n",
        "    origin = bbox_norm[0]\n",
        "    delta_bbox = bbox_norm - origin # change from every bbox from origin\n",
        "\n",
        "    for i, joint in enumerate(joints):\n",
        "      joints[i, :, 0] = joints[i, :, 0] - delta_bbox[i, 0]/2  # estimated x movement\n",
        "      joints[i, :, 1] = joints[i, :, 1] - delta_bbox[i, 1]/2  # estimated y movement\n",
        "\n",
        "    # then, translate pose to be standing on the floor\n",
        "    # lowest y coordinate of feet joints (= 10 & 11)\n",
        "    lowest = min(np.min(joints[:, 10, 1]), np.min(joints[:, 11, 1]))\n",
        "    diff = 0 - lowest\n",
        "    joints[:, :, 1] = joints[:, :, 1] + diff\n",
        "\n",
        "    # finally, smooth pose with smoothing filter\n",
        "    joints = savgol_filter(joints, window_length=12, polyorder=3, axis=0)\n",
        "\n",
        "    return joints\n",
        "\n",
        "  def run(self):\n",
        "    try:\n",
        "      index_df = pd.read_csv(osp.join(self.data_dir, 'index.csv'))\n",
        "    except:\n",
        "      index_df = pd.DataFrame()\n",
        "\n",
        "    for i, video in self.videos_df.iterrows():\n",
        "      print(f'Processing video {i}...')\n",
        "\n",
        "      video_file = video['video_file']\n",
        "      if video['processed'] == True:\n",
        "        video_fps = video['fps']\n",
        "        print(f'video {i} is already processed')\n",
        "        continue\n",
        "      elif any(yt in video_file for yt in [\"youtube\", \"youtu.be\"]):\n",
        "        try:\n",
        "          video_file, video_fps = self.download_yt_video(video_file, i)\n",
        "        except Exception as e:\n",
        "          print(f'Could not download video {i}, {e}')\n",
        "          continue\n",
        "\n",
        "      vibe_output = self.get_vibe_poses(video_file)\n",
        "\n",
        "      caps = self.get_whisper_captions(video_file)\n",
        "      processed_caps = self.process_captions(caps)\n",
        "\n",
        "      index_df = self.align_captions_poses(processed_caps, vibe_output, index_df, i)\n",
        "      self.videos_df.loc[i, ('processed')] = True\n",
        "      self.videos_df.loc[i, ('fps')] = video_fps\n",
        "\n",
        "      # update dfs after each video\n",
        "      index_df.to_csv(osp.join(self.data_dir, 'index.csv'), index=False)\n",
        "      self.videos_df.to_csv(osp.join(self.data_dir,'video_files.csv'), index=False)\n",
        "\n",
        "      vibe_output = caps = processed_caps = None # does this clear RAM?\n",
        "\n",
        "    return index_df"
      ],
      "metadata": {
        "id": "ttAIu1bd5fUd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "2p4Mz4npTHpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "playlist = 'https://www.youtube.com/playlist?list=PLtWL_OMHER4XstPsqS5y-nHUURORdtUkI' # @param {type: \"string\"}\n",
        "# also possible: link to a folder with .mp4 videos or a link to a single yt video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnu6b9InUZKM",
        "outputId": "ec6c0084-5518-461a-ee9a-eda62622d594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create .csv of youtube links.\n",
        "if any(yt in playlist for yt in [\"youtube\", \"youtu.be\"]):\n",
        "  video_list = Playlist(playlist) if 'playlist' in playlist else [playlist]\n",
        "else:\n",
        "  video_list = [f for f in os.listdir(playlist) if 'mp4' in f]\n",
        "print(f'Number Of Videos: {len(video_list)}')\n",
        "\n",
        "videos_df = pd.DataFrame({'video_file':video_list, 'processed':False, 'fps':20})\n",
        "videos_df.to_csv(osp.join(data_dir, 'video_files.csv'), index=False)\n",
        "videos_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "x4yWkqUxTKdx",
        "outputId": "7809a529-925b-4027-eef3-736e16676c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number Of Videos: 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    video_file  processed  fps\n",
              "0  https://www.youtube.com/watch?v=fSHjGPWmkb8      False   20\n",
              "1  https://www.youtube.com/watch?v=XfCK-Kj_ST0      False   20"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e1cac5b-7212-4d03-ac89-fc508cc3ad10\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_file</th>\n",
              "      <th>processed</th>\n",
              "      <th>fps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.youtube.com/watch?v=fSHjGPWmkb8</td>\n",
              "      <td>False</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.youtube.com/watch?v=XfCK-Kj_ST0</td>\n",
              "      <td>False</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e1cac5b-7212-4d03-ac89-fc508cc3ad10')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e1cac5b-7212-4d03-ac89-fc508cc3ad10 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e1cac5b-7212-4d03-ac89-fc508cc3ad10');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a28951c1-05aa-4fae-abd7-5ef725fd49e0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a28951c1-05aa-4fae-abd7-5ef725fd49e0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a28951c1-05aa-4fae-abd7-5ef725fd49e0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dp = DataProcessor(data_dir)"
      ],
      "metadata": {
        "id": "u2m6KDlp-kAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_df = dp.run()"
      ],
      "metadata": {
        "id": "S_CCaNF2_YSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check dataset"
      ],
      "metadata": {
        "id": "nW1of_zSgfm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_df = pd.read_csv(f'{data_dir}/index.csv')\n",
        "index_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KZQoN0CpnwKJ",
        "outputId": "df1edc99-2c33-4515-eef5-79b1e46442d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   file_idx  video_idx  start_frame  end_frame  start_time  end_time  \\\n",
              "0         0          0          149        177        7.45      8.84   \n",
              "1         0          0          149        177        7.45      8.84   \n",
              "2         1          0          177        203        8.84     10.14   \n",
              "3         1          0          177        203        8.84     10.14   \n",
              "4         2          0          203        238       10.14     11.90   \n",
              "\n",
              "      new_name  fps                 caption  no_frames  \n",
              "0   000000.npy   20                     NaN         29  \n",
              "1  M000000.npy   20                     NaN         29  \n",
              "2   000001.npy   20   breathing of statues,         27  \n",
              "3  M000001.npy   20   breathing of statues,         27  \n",
              "4   000002.npy   20                     NaN         36  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03eb12a7-919a-455e-8424-7b0404d5807c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_idx</th>\n",
              "      <th>video_idx</th>\n",
              "      <th>start_frame</th>\n",
              "      <th>end_frame</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>new_name</th>\n",
              "      <th>fps</th>\n",
              "      <th>caption</th>\n",
              "      <th>no_frames</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>149</td>\n",
              "      <td>177</td>\n",
              "      <td>7.45</td>\n",
              "      <td>8.84</td>\n",
              "      <td>000000.npy</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>149</td>\n",
              "      <td>177</td>\n",
              "      <td>7.45</td>\n",
              "      <td>8.84</td>\n",
              "      <td>M000000.npy</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>177</td>\n",
              "      <td>203</td>\n",
              "      <td>8.84</td>\n",
              "      <td>10.14</td>\n",
              "      <td>000001.npy</td>\n",
              "      <td>20</td>\n",
              "      <td>breathing of statues,</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>177</td>\n",
              "      <td>203</td>\n",
              "      <td>8.84</td>\n",
              "      <td>10.14</td>\n",
              "      <td>M000001.npy</td>\n",
              "      <td>20</td>\n",
              "      <td>breathing of statues,</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>203</td>\n",
              "      <td>238</td>\n",
              "      <td>10.14</td>\n",
              "      <td>11.90</td>\n",
              "      <td>000002.npy</td>\n",
              "      <td>20</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03eb12a7-919a-455e-8424-7b0404d5807c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03eb12a7-919a-455e-8424-7b0404d5807c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03eb12a7-919a-455e-8424-7b0404d5807c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-08821fd7-fc9f-4be5-8945-4583579b4917\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08821fd7-fc9f-4be5-8945-4583579b4917')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-08821fd7-fc9f-4be5-8945-4583579b4917 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_frames = sum(index_df['no_frames'])\n",
        "fps = 20\n",
        "print(f'{len(index_df)} files (incl. mirrored), {round((total_frames/fps)/60, 2)} minutes == {round((total_frames/fps)/3600, 2)} hours of movement')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0o6kcD_XHmC",
        "outputId": "72825b2d-1284-4b82-c167-43b5c8d92cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56 files (incl. mirrored), 1.97 minutes == 0.03 hours of movement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(f'{data_dir}/joints/' + os.listdir(f'{data_dir}/joints')[0])\n",
        "data.shape  # should be (n, 24, 3), where n is the number of frames for the sequence."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDJE4gzgQX0c",
        "outputId": "a04856d7-97e7-41ca-afce-60120c83189e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(93, 24, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_df['fps'].unique() # I think this part is not working, all have the same fps..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn9C7N8fZjrZ",
        "outputId": "22742712-196b-48aa-cb85-0755c7201548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run this if you want to reset things"
      ],
      "metadata": {
        "id": "M1Fx8gIFgjVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm {data_dir}/index.csv\n",
        "\n",
        "videos_df = pd.read_csv(f'{data_dir}/video_files.csv')\n",
        "videos_df.loc[1, ('processed')] = False  # TODO: do this for every video\n",
        "videos_df.to_csv(osp.join(data_dir,'video_files.csv'), index=False)\n",
        "print(videos_df.head())\n",
        "\n",
        "!rm {data_dir}/joints/*"
      ],
      "metadata": {
        "id": "CPdKEcShTOAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualise motions\n",
        "To check if everything went well, you can visualise one of the motions."
      ],
      "metadata": {
        "id": "J7FPusS_Hlfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import join as pjoin\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "import mpl_toolkits.mplot3d.axes3d as p3\n",
        "import joblib\n",
        "\n",
        "kinematic_chain = [[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]\n",
        "\n",
        "def plot_3d_motion(save_path, kinematic_tree, joints, title, figsize=(10, 10), fps=120, radius=4):\n",
        "\n",
        "    title_sp = title.split(' ')\n",
        "    if len(title_sp) > 10:\n",
        "        title = '\\n'.join([' '.join(title_sp[:10]), ' '.join(title_sp[10:])])\n",
        "    def init():\n",
        "        ax.set_xlim3d([-radius / 2, radius / 2])\n",
        "        ax.set_ylim3d([0, radius])\n",
        "        ax.set_zlim3d([0, radius])\n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        ax.grid(b=False)\n",
        "\n",
        "    def plot_xzPlane(minx, maxx, miny, minz, maxz):\n",
        "        ## Plot a plane XZ\n",
        "        verts = [\n",
        "            [minx, miny, minz],\n",
        "            [minx, miny, maxz],\n",
        "            [maxx, miny, maxz],\n",
        "            [maxx, miny, minz]\n",
        "        ]\n",
        "        xz_plane = Poly3DCollection([verts])\n",
        "        xz_plane.set_facecolor((0.5, 0.5, 0.5, 0.5))\n",
        "        ax.add_collection3d(xz_plane)\n",
        "\n",
        "    data = joints.copy().reshape(len(joints), -1, 3)\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax = p3.Axes3D(fig)\n",
        "    init()\n",
        "    MINS = data.min(axis=0).min(axis=0)\n",
        "    MAXS = data.max(axis=0).max(axis=0)\n",
        "    colors = ['red', 'blue', 'black', 'red', 'blue',\n",
        "              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',\n",
        "             'darkred', 'darkred','darkred','darkred','darkred']\n",
        "    frame_number = data.shape[0]\n",
        "\n",
        "    height_offset = MINS[1]\n",
        "    data[:, :, 1] -= height_offset\n",
        "    trajec = data[:, 0, [0, 2]]\n",
        "\n",
        "    data[..., 0] -= data[:, 0:1, 0]\n",
        "    data[..., 2] -= data[:, 0:1, 2]\n",
        "\n",
        "    def update(index):\n",
        "        ax.lines = []\n",
        "        ax.collections = []\n",
        "        ax.view_init(elev=120, azim=-90)\n",
        "        ax.dist = 7.5\n",
        "        plot_xzPlane(MINS[0]-trajec[index, 0], MAXS[0]-trajec[index, 0], 0, MINS[2]-trajec[index, 1], MAXS[2]-trajec[index, 1])\n",
        "\n",
        "        if index > 1:\n",
        "            ax.plot3D(trajec[:index, 0]-trajec[index, 0], np.zeros_like(trajec[:index, 0]), trajec[:index, 1]-trajec[index, 1], linewidth=1.0,\n",
        "                      color='blue')\n",
        "\n",
        "\n",
        "        for i, (chain, color) in enumerate(zip(kinematic_tree, colors)):\n",
        "            if i < 5:\n",
        "                linewidth = 4.0\n",
        "            else:\n",
        "                linewidth = 2.0\n",
        "            ax.plot3D(data[index, chain, 0], data[index, chain, 1], data[index, chain, 2], linewidth=linewidth, color=color)\n",
        "\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_zticklabels([])\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=frame_number, interval=1000/fps, repeat=False)\n",
        "\n",
        "    ani.save(save_path, fps=fps)\n",
        "    print('animation is saved')\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "cyQhcnbMHmvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/animation.mp4' #@param {type: 'string'}\n",
        "joints = np.load(f'{data_dir}/joints/' + os.listdir(f'{data_dir}/joints')[0])\n",
        "plot_3d_motion(save_path, kinematic_chain, joints, title=\" \", fps=25, radius=4)"
      ],
      "metadata": {
        "id": "rx70S6jDHrOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "923defb0-3914-4681-934d-606fa244042a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "animation is saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post process with HumanML3D\n",
        "\n",
        "Code largely taken from HumanML3D's  `motion_representation.ipynb`, somewhat edited/cleaned."
      ],
      "metadata": {
        "id": "lBZtHrgmpUKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/EricGuo5513/HumanML3D.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O65p9L_xpXtU",
        "outputId": "1ce3f6ac-e416-4757-bc63-f29cc1979aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'content'\n",
            "/content\n",
            "Cloning into 'HumanML3D'...\n",
            "remote: Enumerating objects: 192, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 192 (delta 46), reused 35 (delta 27), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (192/192), 132.74 MiB | 15.94 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "%cd HumanML3D\n",
        "from os.path import join as pjoin\n",
        "\n",
        "from common.skeleton import Skeleton\n",
        "import numpy as np\n",
        "import os\n",
        "from common.quaternion import *\n",
        "from paramUtil import *\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "T5hD8KynpWsw",
        "outputId": "81965cbe-1c81-4bf2-9f4d-4a4d8a9babb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HumanML3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Util functions\n",
        "def uniform_skeleton(positions, tgt_offset):\n",
        "\n",
        "  skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
        "  src_offset = skel.get_offsets_joints(torch.from_numpy(positions[0]))\n",
        "\n",
        "  src_leg_len = np.abs(src_offset[5]).max() + np.abs(src_offset[8]).max() # lower legs = 5, 8\n",
        "  tgt_leg_len = np.abs(tgt_offset[5]).max() + np.abs(tgt_offset[8]).max()\n",
        "\n",
        "  scale_rt = tgt_leg_len / src_leg_len\n",
        "  tgt_root_pos = torch.from_numpy(positions[:, 0]) * scale_rt\n",
        "\n",
        "  quat_params = skel.inverse_kinematics_np(positions, face_joint_indx)\n",
        "  skel.set_offset(tgt_offset)\n",
        "  new_joints = skel.forward_kinematics_np(quat_params, tgt_root_pos)\n",
        "  return new_joints\n",
        "\n",
        "\n",
        "def recover_root_rot_pos(data):\n",
        "  rot_vel = data[..., 0]\n",
        "  r_rot_ang = torch.cumsum(torch.cat([torch.zeros_like(rot_vel[..., :1]), rot_vel[..., :-1]], dim=-1), dim=-1)\n",
        "\n",
        "  r_rot_quat = torch.zeros(data.shape[:-1] + (4,)).to(data.device)\n",
        "  r_rot_quat[..., 0] = torch.cos(r_rot_ang)\n",
        "  r_rot_quat[..., 2] = torch.sin(r_rot_ang)\n",
        "\n",
        "  r_pos = torch.zeros(data.shape[:-1] + (3,)).to(data.device)\n",
        "  r_pos[..., 1:, [0, 2]] = data[..., :-1, 1:3]\n",
        "\n",
        "  r_rot_quat_inv = qinv(r_rot_quat)\n",
        "  r_pos = qrot(r_rot_quat_inv, r_pos)\n",
        "  r_pos = torch.cumsum(r_pos, dim=-2)\n",
        "  r_pos[..., 1] = data[..., 3]\n",
        "\n",
        "  return r_rot_quat, r_pos\n",
        "\n",
        "\n",
        "def recover_from_rot(data, joints_num, skeleton):\n",
        "  r_rot_quat, r_pos = recover_root_rot_pos(data)\n",
        "  r_rot_cont6d = quaternion_to_cont6d(r_rot_quat)\n",
        "\n",
        "  start_indx = 1 + 2 + 1 + (joints_num - 1) * 3\n",
        "  end_indx = start_indx + (joints_num - 1) * 6\n",
        "  cont6d_params = data[..., start_indx:end_indx]\n",
        "  cont6d_params = data[..., 1:]\n",
        "  cont6d_params = torch.cat([r_rot_cont6d, cont6d_params], dim=-1)\n",
        "  cont6d_params = cont6d_params.view(-1, joints_num, 6)\n",
        "\n",
        "  positions = skeleton.forward_kinematics_cont6d(cont6d_params, r_pos)\n",
        "\n",
        "  return positions\n",
        "\n",
        "\n",
        "def recover_from_ric(data, joints_num):\n",
        "  r_rot_quat, r_pos = recover_root_rot_pos(data)\n",
        "  positions = data[..., 4:(joints_num - 1) * 3 + 4]\n",
        "  positions = positions.view(positions.shape[:-1] + (-1, 3))\n",
        "\n",
        "  positions = qrot(qinv(r_rot_quat[..., None, :]).expand(positions.shape[:-1] + (4,)), positions)\n",
        "\n",
        "  positions[..., 0] += r_pos[..., 0:1]\n",
        "  positions[..., 2] += r_pos[..., 2:3]\n",
        "\n",
        "  positions = torch.cat([r_pos.unsqueeze(-2), positions], dim=-2)\n",
        "\n",
        "  return positions\n",
        "\n",
        "def process_file(positions, feet_thre):\n",
        "    # floor\n",
        "    positions = uniform_skeleton(positions, tgt_offsets)\n",
        "    floor_height = positions.min(axis=0).min(axis=0)[1]\n",
        "    positions[:, :, 1] -= floor_height\n",
        "\n",
        "    # XZ at origin\n",
        "    root_pos_init = positions[0]\n",
        "    positions = positions - root_pos_init[0] * np.array([1, 0, 1])\n",
        "\n",
        "    # Z+\n",
        "    # Unpack face joint indices\n",
        "    # r_hip, l_hip, sdr_r, sdr_l = face_joint_indx\n",
        "\n",
        "    # Calculate the direction across the hips and shoulders\n",
        "    # across1 = root_pos_init[r_hip] - root_pos_init[l_hip]\n",
        "    # across2 = root_pos_init[sdr_r] - root_pos_init[sdr_l]\n",
        "    # across = (across1 + across2) / np.sqrt((across1 + across2) ** 2).sum(axis=-1)[..., np.newaxis]\n",
        "\n",
        "    # # Calculate forward direction\n",
        "    # forward_init = np.cross(np.array([[0, 1, 0]]), across, axis=-1)\n",
        "    # forward_init /= np.sqrt((forward_init ** 2).sum(axis=-1))[..., np.newaxis]\n",
        "\n",
        "    # Set the target direction\n",
        "    # target = np.array([[0, 0, 1]])\n",
        "\n",
        "    # # Calculate the initial root quaternion\n",
        "    # root_quat_init = qbetween_np(forward_init, target)\n",
        "    # root_quat_init = np.ones(positions.shape[:-1] + (4,)) * root_quat_init\n",
        "\n",
        "    # Rotate positions using the root quaternion\n",
        "    # positions = qrot_np(root_quat_init, positions)\n",
        "    global_positions = positions.copy()\n",
        "\n",
        "\n",
        "    def foot_detect(positions, thres):\n",
        "      # Calculate feet detection thresholds\n",
        "      velfactor, heightfactor = np.array([thres, thres]), np.array([3.0, 2.0])\n",
        "\n",
        "      # Calculate left feet detection\n",
        "      feet_l_x = (positions[1:, fid_l, 0] - positions[:-1, fid_l, 0]) ** 2\n",
        "      feet_l_y = (positions[1:, fid_l, 1] - positions[:-1, fid_l, 1]) ** 2\n",
        "      feet_l_z = (positions[1:, fid_l, 2] - positions[:-1, fid_l, 2]) ** 2\n",
        "      feet_l = ((feet_l_x + feet_l_y + feet_l_z) < velfactor).astype(np.float32)\n",
        "\n",
        "      # Calculate right feet detection\n",
        "      feet_r_x = (positions[1:, fid_r, 0] - positions[:-1, fid_r, 0]) ** 2\n",
        "      feet_r_y = (positions[1:, fid_r, 1] - positions[:-1, fid_r, 1]) ** 2\n",
        "      feet_r_z = (positions[1:, fid_r, 2] - positions[:-1, fid_r, 2]) ** 2\n",
        "      feet_r = ((feet_r_x + feet_r_y + feet_r_z) < velfactor).astype(np.float32)\n",
        "\n",
        "      return feet_l, feet_r\n",
        "\n",
        "    feet_l, feet_r = foot_detect(positions, feet_thre)\n",
        "    r_rot = None\n",
        "\n",
        "    def get_rifke(positions):\n",
        "      positions[..., 0] -= positions[:, 0:1, 0]\n",
        "      positions[..., 2] -= positions[:, 0:1, 2]\n",
        "      positions = qrot_np(np.repeat(r_rot[:, None], positions.shape[1], axis=1), positions)\n",
        "      return positions\n",
        "\n",
        "    def get_cont6d_params(positions):\n",
        "      skel = Skeleton(n_raw_offsets, kinematic_chain, \"cpu\")\n",
        "      quat_params = skel.inverse_kinematics_np(positions, face_joint_indx, smooth_forward=False)\n",
        "\n",
        "      # Quaternion to continuous 6D\n",
        "      cont_6d_params = quaternion_to_cont6d_np(quat_params)\n",
        "      r_rot = quat_params[:, 0].copy()\n",
        "\n",
        "      # Root Linear Velocity\n",
        "      velocity = (positions[1:, 0] - positions[:-1, 0]).copy()\n",
        "      velocity = qrot_np(r_rot[1:], velocity)\n",
        "\n",
        "      # Root Angular Velocity\n",
        "      r_velocity = qmul_np(r_rot[1:], qinv_np(r_rot[:-1]))\n",
        "\n",
        "      return cont_6d_params, r_velocity, velocity, r_rot\n",
        "\n",
        "    cont_6d_params, r_velocity, velocity, r_rot = get_cont6d_params(positions)\n",
        "    positions = get_rifke(positions)\n",
        "\n",
        "    # Root height\n",
        "    root_y = positions[:, 0, 1:2]\n",
        "\n",
        "    # Root rotation and linear velocity\n",
        "    r_velocity = np.arcsin(r_velocity[:, 2:3])\n",
        "    l_velocity = velocity[:, [0, 2]]\n",
        "    root_data = np.concatenate([r_velocity, l_velocity, root_y[:-1]], axis=-1)\n",
        "\n",
        "    # Get Joint Rotation Representation\n",
        "    rot_data = cont_6d_params[:, 1:].reshape(len(cont_6d_params), -1)\n",
        "\n",
        "    # Get Joint Rotation Invariant Position Representation\n",
        "    ric_data = positions[:, 1:].reshape(len(positions), -1)\n",
        "\n",
        "    # Get Joint Velocity Representation\n",
        "    local_vel = qrot_np(np.repeat(r_rot[:-1, None], global_positions.shape[1], axis=1),\n",
        "                        global_positions[1:] - global_positions[:-1])\n",
        "    local_vel = local_vel.reshape(len(local_vel), -1)\n",
        "\n",
        "    # Combine all representations into a single data array\n",
        "    data = np.concatenate([root_data, ric_data[:-1], rot_data[:-1], local_vel, feet_l, feet_r], axis=-1)\n",
        "\n",
        "    return data, global_positions, positions, l_velocity"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qXunZC3ap1nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run motion_representation\n",
        "\n",
        "example_id = \"000000\"\n",
        "fid_r, fid_l = [8, 11], [7, 10]\n",
        "face_joint_indx = [2, 1, 17, 16]\n",
        "r_hip, l_hip = 2, 1\n",
        "joints_num = 22\n",
        "\n",
        "joints_dir = f'{data_dir}/joints/'\n",
        "new_joints_dir = f'{data_dir}/new_joints/'\n",
        "new_joint_vecs_dir = f'{data_dir}/new_joint_vecs/'\n",
        "\n",
        "os.makedirs(new_joints_dir, exist_ok=True)\n",
        "os.makedirs(new_joint_vecs_dir, exist_ok=True)\n",
        "\n",
        "n_raw_offsets = torch.from_numpy(t2m_raw_offsets)\n",
        "kinematic_chain = t2m_kinematic_chain\n",
        "\n",
        "# Get offsets of target skeleton\n",
        "example_data = np.load(os.path.join(joints_dir, example_id + '.npy'))\n",
        "example_data = example_data.reshape(len(example_data), -1, 3)\n",
        "example_data = torch.from_numpy(example_data)\n",
        "tgt_skel = Skeleton(n_raw_offsets, kinematic_chain, 'cpu')\n",
        "tgt_offsets = tgt_skel.get_offsets_joints(example_data[0])\n",
        "\n",
        "source_list = sorted(os.listdir(joints_dir))\n",
        "frame_num = 0\n",
        "\n",
        "for source_file in tqdm(source_list):\n",
        "    source_data = np.load(os.path.join(joints_dir, source_file))[:, :joints_num]\n",
        "    try:\n",
        "      data, ground_positions, positions, l_velocity = process_file(source_data, 0.002)\n",
        "      rec_ric_data = recover_from_ric(torch.from_numpy(data).unsqueeze(0).float(), joints_num)\n",
        "      np.save(pjoin(new_joints_dir, source_file), rec_ric_data.squeeze().numpy())\n",
        "      np.save(pjoin(new_joint_vecs_dir, source_file), data)\n",
        "      frame_num += data.shape[0]\n",
        "    except Exception as e:\n",
        "        print(source_file)\n",
        "        print(e)\n",
        "\n",
        "print('Total clips: %d, Frames: %d, Duration: %fm' % (len(source_list), frame_num, frame_num / 20 / 60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "DeYt98o0qBMi",
        "outputId": "3ba78811-15ec-4e67-e658-7aa55d0fea1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 56/56 [00:03<00:00, 15.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total clips: 56, Frames: 2304, Duration: 1.920000m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title mean_variance code\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from os.path import join as pjoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Info:\n",
        "# root_rot_velocity (B, seq_len, 1)\n",
        "# root_linear_velocity (B, seq_len, 2)\n",
        "# root_y (B, seq_len, 1)\n",
        "# ric_data (B, seq_len, (joint_num - 1)*3)\n",
        "# rot_data (B, seq_len, (joint_num - 1)*6)\n",
        "# local_velocity (B, seq_len, joint_num*3)\n",
        "# foot contact (B, seq_len, 4)\n",
        "\n",
        "def mean_variance(data_dir, save_dir, joints_num):\n",
        "    file_list = os.listdir(data_dir)\n",
        "    data_list = []\n",
        "\n",
        "    for file in tqdm(file_list, desc='reading files'):\n",
        "        data = np.load(pjoin(data_dir, file))\n",
        "        if np.isnan(data).any():\n",
        "            print(file)\n",
        "            continue\n",
        "        data_list.append(data)\n",
        "\n",
        "    data = np.concatenate(data_list, axis=0)\n",
        "    print(data.shape)\n",
        "    Mean = data.mean(axis=0)\n",
        "    Std = data.std(axis=0)\n",
        "    Std[0:1] = Std[0:1].mean() / 1.0\n",
        "    Std[1:3] = Std[1:3].mean() / 1.0\n",
        "    Std[3:4] = Std[3:4].mean() / 1.0\n",
        "    Std[4: 4+(joints_num - 1) * 3] = Std[4: 4+(joints_num - 1) * 3].mean() / 1.0\n",
        "    Std[4+(joints_num - 1) * 3: 4+(joints_num - 1) * 9] = Std[4+(joints_num - 1) * 3: 4+(joints_num - 1) * 9].mean() / 1.0\n",
        "    Std[4+(joints_num - 1) * 9: 4+(joints_num - 1) * 9 + joints_num*3] = Std[4+(joints_num - 1) * 9: 4+(joints_num - 1) * 9 + joints_num*3].mean() / 1.0\n",
        "    Std[4 + (joints_num - 1) * 9 + joints_num * 3: ] = Std[4 + (joints_num - 1) * 9 + joints_num * 3: ].mean() / 1.0\n",
        "\n",
        "    assert 8 + (joints_num - 1) * 9 + joints_num * 3 == Std.shape[-1]\n",
        "\n",
        "    np.save(pjoin(save_dir, 'Mean.npy'), Mean)\n",
        "    np.save(pjoin(save_dir, 'Std.npy'), Std)\n",
        "\n",
        "    return Mean, Std"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JwIzIxNHqZPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_joint_vecs_dir = f'{data_dir}/new_joint_vecs/'\n",
        "mean, std = mean_variance(new_joint_vecs_dir, data_dir, 22)\n",
        "\n",
        "# See if you're on the right track:\n",
        "HML3D_mean = np.load('HumanML3D/Mean.npy')\n",
        "HML3D_std = np.load('HumanML3D/Std.npy')\n",
        "print(f'Mean mine: {np.mean(mean)}, HML3D: {np.mean(HML3D_mean)}')\n",
        "print(f'Std mine: {np.std(std)}, HML3D: {np.std(HML3D_std)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jW_yNPRsGyP",
        "outputId": "bc1f2ad4-7dd4-46c6-ed0f-03ab3e1fc234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading files: 100%|| 56/56 [00:00<00:00, 597.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2304, 263)\n",
            "Mean mine: 0.17349901795387268, HML3D: 0.1882529854774475\n",
            "Std mine: 0.15011994540691376, HML3D: 0.12204898148775101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: train/test split"
      ],
      "metadata": {
        "id": "R6UxdKT0sqxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qbv8kPnLssB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}